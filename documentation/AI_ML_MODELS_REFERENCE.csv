Model Category,Model Name,Size Classification,Parameter Count,Memory Footprint,Library/Framework,Version Requirements,Primary Purpose,Implementation Files,Configuration Details,Input Specification,Output Specification,Performance Characteristics,Runtime Dependencies,Training Requirements,Inference Speed,Model Architecture Details
Large Language Model,all-MiniLM-L6-v2,Small-Medium,22.7M parameters,90-120 MB RAM,sentence-transformers,>=2.0.0,Primary text embedding generation,core_system.py utils.py query_processor.py,Config.EMBEDDING_MODEL='all-MiniLM-L6-v2',Raw text strings (max 512 tokens),384-dimensional dense vectors,Optimized for semantic similarity tasks,transformers torch pytorch,Pre-trained on Microsoft datasets,~50ms per embedding,6-layer BERT-based transformer
Large Language Model,all-MiniLM-L6-v2,Small-Medium,22.7M parameters,90-120 MB RAM,sentence-transformers,>=2.0.0,Intent classification via embeddings,core_system.py QueryUnderstandingEngine._classify_intent_nlp(),Pre-computed intent embeddings cached,Query strings,Cosine similarity scores,Sub-millisecond classification,transformers torch pytorch,Pre-trained on Microsoft datasets,<1ms inference,Dense retrieval architecture
Large Language Model,all-MiniLM-L6-v2,Small-Medium,22.7M parameters,90-120 MB RAM,sentence-transformers,>=2.0.0,Query expansion and similarity,core_system.py._expand_query_nlp(),NLP-based query enhancement,Keyword lists,Enhanced query terms,Semantic similarity threshold 0.7,transformers torch pytorch,Pre-trained on Microsoft datasets,~20ms per expansion,Symmetric sentence embedding
Large Language Model,all-MiniLM-L6-v2,Small-Medium,22.7M parameters,90-120 MB RAM,sentence-transformers,>=2.0.0,AI-powered property summarization,core_system.py AISummaryGenerator,Lightweight summarization model,Property JSON documents,Intelligent property summaries,Context-aware content generation,transformers torch pytorch,Pre-trained on Microsoft datasets,~100ms per summary,Encoder-only transformer
NLP Pipeline,en_core_web_sm,Medium,50M parameters,200-250 MB RAM,spacy,>=3.0.0,Named Entity Recognition,core_system.py utils.py TextProcessor,Optional advanced NLP processing,Raw text documents,Named entities with labels,Industrial-strength NLP accuracy,None (standalone),Trained on OntoNotes 5.0,~10ms per document,CNN + BiLSTM architecture
NLP Pipeline,en_core_web_sm,Medium,50M parameters,200-250 MB RAM,spacy,>=3.0.0,Part-of-speech tagging,core_system.py._extract_entities_nlp(),Advanced linguistic analysis,Text sequences,POS tags and dependencies,99%+ accuracy on standard datasets,None (standalone),Trained on Universal Dependencies,~5ms per sentence,Multi-task learning architecture
NLP Pipeline,en_core_web_sm,Medium,50M parameters,200-250 MB RAM,spacy,>=3.0.0,Advanced text preprocessing,utils.py TextProcessor._load_nlp_model(),Context-aware text cleaning,Raw user input,Cleaned linguistic tokens,Better than regex-based processing,None (standalone),Trained on web text corpora,~8ms per text,Statistical + neural components
Vector Database,FAISS IndexFlatIP,Minimal,N/A (index structure),Variable by dataset size,faiss-cpu or faiss-gpu,>=1.6.0,High-speed vector similarity search,utils.py IndexManager core_system.py,Inner product for cosine similarity,384-dim float32 vectors,Document IDs and similarity scores,Sub-second search on 50K+ documents,numpy blas,No training required,<100ms for 10K docs,Flat exhaustive search
Vector Database,FAISS IndexFlatIP,Minimal,N/A (index structure),Variable by dataset size,faiss-cpu or faiss-gpu,>=1.6.0,Semantic document retrieval,core_system.py SemanticSearchEngine._perform_faiss_search(),FAISS index with L2 normalization,Query embedding vectors,Top-k nearest neighbors,Linear search complexity O(n),numpy blas,No training required,Scales linearly,Brute-force similarity
Math Library,NumPy Arrays,Minimal,N/A (data structure),Depends on embedding count,numpy,>=1.19.0,Dense vector storage and operations,All embedding operations,Float32 arrays for memory efficiency,Document text,Dense vector representations,Optimized BLAS operations,BLAS LAPACK,No training required,Microseconds,Vectorized computations
Math Library,NumPy Linear Algebra,Minimal,N/A (algorithms),Temporary memory only,numpy,>=1.19.0,Vector similarity calculations,core_system.py cosine similarity,BLAS-optimized linear algebra,Vector pairs,Cosine similarity scores,Hardware-accelerated math,BLAS LAPACK,No training required,Nanoseconds per operation,SIMD vectorization
Pattern Engine,Python RegEx,Minimal,N/A (finite automaton),<1 MB memory,re,Built-in standard library,Numeric constraint extraction,query_processor.py config/numeric_config.py,Compiled pattern objects,Query strings,Extracted numeric values,Deterministic pattern matching,None,No training required,Microseconds per match,Finite state machine
Pattern Engine,Python RegEx,Minimal,N/A (finite automaton),<1 MB memory,re,Built-in standard library,Entity pattern recognition,query_processor.py EntityExtractor,Property type and amenity patterns,Natural language text,Named entity matches,Fallback when SpaCy unavailable,None,No training required,Sub-millisecond,Regular expression engine
String Matcher,FuzzyWuzzy Levenshtein,Small,N/A (algorithm),<10 MB memory,fuzzywuzzy,>=0.18.0,Approximate string matching,utils.py core_system.py,Levenshtein distance computation,String pairs,Similarity percentages (0-100),Handles typos and variations,python-levenshtein,No training required,~1ms per comparison,Edit distance algorithm
Transformer Core,BERT Architecture,Medium,22.7M base parameters,90 MB base model,transformers,>=4.0.0,Neural text encoding foundation,sentence-transformers backend,Pre-trained BERT weights,Tokenized sequences,Contextualized embeddings,Attention-based encoding,torch transformers tokenizers,Pre-trained on BookCorpus + Wikipedia,Variable by sequence length,Multi-head self-attention
Compute Backend,PyTorch Framework,Large,N/A (framework),Varies by model usage,torch,>=1.9.0,Neural network computation engine,All transformer operations,GPU/CPU automatic selection,Tensor operations,Model predictions,GPU acceleration when available,CUDA (optional),No training required,Hardware dependent,Dynamic computation graphs
Tokenization,BERT Tokenizer,Small,30K vocab + embeddings,~50 MB memory,transformers,>=4.0.0,Text tokenization for embeddings,sentence-transformers internal,WordPiece vocabulary,Raw text input,Sub-word token sequences,Handles out-of-vocabulary terms,None,Pre-trained vocabulary,<1ms per text,Byte-pair encoding variant
Caching System,Pickle Serialization,Variable,N/A (serialization),Matches original data size,pickle,Built-in standard library,Embedding persistence and caching,utils.py EmbeddingGenerator,Binary object serialization,Python objects (numpy arrays),Cached embedding objects,Avoids model recomputation,None,No training required,I/O bound,Binary serialization protocol
Database Interface,PyMongo Client,Minimal,N/A (driver),<50 MB memory,pymongo,>=3.12.0,MongoDB document database access,utils.py DatabaseConnector,Connection pooling and auth,MongoDB queries,Document dictionaries,Efficient document retrieval,None,No training required,Network latency bound,MongoDB wire protocol
Search Algorithm,FAISS Search Engine,Minimal,N/A (algorithm),Index dependent,faiss-cpu,>=1.6.0,Approximate nearest neighbor search,utils.py IndexManager.search_similar_documents(),Optimized similarity search,Query vectors,Ranked neighbor lists,Sublinear search complexity,numpy,No training required,<100ms typical,Optimized similarity algorithms
NLP Pipeline,SpaCy Complete Pipeline,Medium,50M total parameters,200-250 MB memory,spacy,>=3.0.0,End-to-end language processing,core_system.py QueryUnderstandingEngine,Full linguistic analysis pipeline,Raw text documents,Rich linguistic annotations,Production-grade NLP accuracy,None,Multi-dataset training,~15ms per document,Hybrid statistical + neural
Vector Operations,BLAS Implementation,System-dependent,N/A (optimized routines),Minimal overhead,OS BLAS library,System provided,High-performance linear algebra,numpy backend operations,Hardware-optimized routines,Numerical arrays,Mathematical results,Near-peak hardware performance,Hardware-specific,Optimized assembly,Hardware dependent,Processor-specific optimization
Text Cleaner,ASCII Filter,Minimal,N/A (character mapping),Negligible memory,Built-in standard library,Python built-in,Unicode to ASCII conversion,query_processor.py utils.py,Character encoding normalization,Unicode text input,ASCII-safe text output,Prevents encoding issues,None,No training required,Nanoseconds per character,Character-by-character filtering
Classifier,Pattern-Based Intent Classifier,Small,N/A (rule patterns),<5 MB memory,Built-in standard library,Python built-in,Intent classification via patterns,core_system.py QueryUnderstandingEngine,Hand-crafted classification rules,Query strings,Intent labels with confidence,Fast deterministic classification,None,Hand-crafted rules,Sub-millisecond,Rule-based decision tree
Ranking Algorithm,Weighted Score Combiner,Minimal,N/A (mathematical function),Negligible memory,Built-in standard library,Python built-in,Multi-source result ranking,core_system.py SemanticSearchEngine,Configurable weight parameters,Multiple result lists,Unified ranked results,Customizable relevance weights,None,Mathematical formula,Microseconds,Weighted linear combination
Constraint Solver,Tolerance-Based Matcher,Minimal,N/A (comparison logic),Minimal memory,Built-in standard library,Python built-in,Numeric constraint satisfaction,core_system.py._apply_intelligent_constraints(),Configurable tolerance levels,Numeric constraints,Boolean satisfaction results,Flexible constraint handling,None,Domain-specific logic,Nanoseconds per constraint,Numerical comparison with tolerance
Session Storage,Context Manager,Minimal,N/A (data structure),Scales with conversation length,Built-in standard library,Python built-in,Conversation context tracking,core_system.py SessionManager,Dictionary-based storage,Conversation history,Contextual information,Maintains conversational state,None,No training required,Memory access speed,Hash table implementation
Template Engine,Response Formatter,Minimal,N/A (string operations),Minimal memory,Built-in standard library,Python built-in,Response text generation,core_system.py ResponseGenerator,String template system,Search results + templates,Formatted user responses,Human-readable output generation,None,Template patterns,Microseconds,String interpolation
Indexing System,Vocabulary Term Indexer,Small,N/A (hash table),Proportional to vocabulary size,Built-in standard library,Python built-in,Term frequency analysis,utils.py VocabularyManager,Hash-based term indexing,Document corpus,Term frequency statistics,Efficient term lookup operations,None,Statistical analysis,Hash table access time,Inverted index structure
Rule Engine,Category Classifier,Small,N/A (decision rules),<1 MB memory,Built-in standard library,Python built-in,Query categorization for optimization,core_system.py AirbnbDataOptimizer,Hand-crafted classification rules,Query feature vectors,Category labels,Optimizes search strategy selection,None,Domain expertise,Sub-millisecond,Decision tree classifier
Hybrid System,Multi-Modal Search Engine,Complex,Combination of above,Sum of component memory,Multiple frameworks,Combined requirements,Unified search across modalities,core_system.py SemanticSearchEngine,Weighted combination strategy,Query + multiple indexes,Ranked unified results,Best-of-breed search capabilities,All above dependencies,No additional training,Combined latency,Ensemble methodology
Domain Adapter,Airbnb-Specific Optimizer,Small,N/A (domain rules),<10 MB memory,Built-in standard library,Python built-in,Property-specific optimization,core_system.py AirbnbDataOptimizer utils.py,Domain-specific mappings,Property queries,Optimized search parameters,Tailored for accommodation search,None,Domain knowledge,Negligible overhead,Rule-based field mapping
,,,,,,,,,,,,,,,,
SpaCy Model,en_core_web_sm,Medium,50M parameters,200 MB,spacy,Latest,Named Entity Recognition,core_system.py utils.py,Optional NLP processing,String text,Document object with entities,Advanced entity extraction when available,None,,,
SpaCy Model,en_core_web_sm,Medium,50M parameters,200 MB,spacy,Latest,Part-of-speech tagging,core_system.py,Context enhancement,String text,POS tags and dependencies,Improves context understanding,None,,,
SpaCy Model,en_core_web_sm,Medium,50M parameters,200 MB,spacy,Latest,Text preprocessing,utils.py TextProcessor,Advanced text processing,String text,Cleaned processed text,Better than regex-based processing,None,,,
FAISS Index,IndexFlatIP,Minimal,N/A,Variable,faiss-cpu,Latest,Vector similarity search,utils.py IndexManager,Inner product similarity,384-dim vectors,Document indices and scores,Optimized for cosine similarity,numpy,,,
FAISS Index,IndexFlatIP,Minimal,N/A,Variable,faiss-cpu,Latest,Fast document retrieval,core_system.py SemanticSearchEngine,FAISS index storage,Query embeddings,Top-k similar documents,Sub-second search on 10K+ docs,numpy,,,
NumPy Arrays,Document Embeddings,Variable,N/A,Variable,numpy,Latest,Embedding storage and computation,utils.py core_system.py,Float32 arrays,Text documents,Dense vector representations,Memory-efficient float32 storage,None,,,
NumPy Arrays,Similarity Matrices,Small,N/A,Minimal,numpy,Latest,Cosine similarity calculation,core_system.py,Dot product operations,Vector pairs,Similarity scores,Vectorized operations for speed,None,,,
RegEx Engine,Python re,Minimal,N/A,Minimal,re,Built-in,Numeric constraint extraction,query_processor.py config/numeric_config.py,Compiled patterns,String queries,Extracted numeric values,Pattern-based extraction,None,,,
RegEx Engine,Python re,Minimal,N/A,Minimal,re,Built-in,Entity pattern matching,query_processor.py,Property type patterns,String queries,Matched entities,Fallback when SpaCy unavailable,None,,,
Fuzzy Matcher,fuzzywuzzy,Small,N/A,Minimal,fuzzywuzzy,Latest,String similarity matching,utils.py core_system.py,Levenshtein distance,String pairs,Similarity scores,Handles typos and variations,python-levenshtein,,,
Transformer Base,Sentence-BERT Architecture,Medium,22.7M,90 MB,sentence-transformers,Latest,Dense passage retrieval,All embedding operations,BERT-based encoding,Text sequences,Dense representations,Optimized for semantic similarity,transformers torch,,,
Torch Backend,PyTorch,Large,N/A,Variable,torch,Latest,Neural network computation,sentence-transformers backend,GPU/CPU inference,Tensor operations,Model predictions,GPU acceleration when available,CUDA optional,,,
Tokenizer,BERT Tokenizer,Small,N/A,10 MB,transformers,Latest,Text tokenization for embeddings,sentence-transformers internal,WordPiece tokenization,Raw text,Token IDs,Handles subword tokenization,None,,,
Embedding Cache,Pickle Files,Variable,N/A,Variable,pickle,Built-in,Embedding persistence,utils.py EmbeddingGenerator,Binary serialization,NumPy arrays,Cached embeddings,Avoids recomputation,None,,,
MongoDB Connector,PyMongo,Minimal,N/A,Minimal,pymongo,Latest,Document database interface,utils.py DatabaseConnector,Database queries,MongoDB collections,Document dictionaries,Efficient document retrieval,None,,,
Searcher,FAISS Search,Minimal,N/A,Minimal,faiss-cpu,Latest,Approximate nearest neighbors,utils.py IndexManager,Index search operations,Query vectors,Nearest neighbor indices,Sublinear search complexity,numpy,,,
NLP Pipeline,SpaCy Pipeline,Medium,50M,200 MB,spacy,Latest,Complete NLP processing,core_system.py,Full pipeline processing,Text documents,Linguistic annotations,Industrial-strength NLP,None,,,
Vector Operations,NumPy Linear Algebra,Minimal,N/A,Minimal,numpy,Built-in,Vector math operations,All vector computations,BLAS-optimized operations,Vector arrays,Mathematical results,Highly optimized linear algebra,BLAS LAPACK,,,
Text Processor,ASCII Filter,Minimal,N/A,Minimal,Built-in,N/A,Unicode handling,query_processor.py utils.py,Character filtering,Unicode text,ASCII-safe text,Prevents encoding issues,None,,,
Query Analyzer,Pattern Matcher,Small,N/A,Minimal,re,Built-in,Intent classification,core_system.py,Rule-based classification,Query strings,Intent labels,Fast pattern-based classification,None,,,
Contextual Booster,Weighted Scorer,Minimal,N/A,Minimal,Built-in,N/A,Result ranking enhancement,core_system.py,Score multiplication,Search results,Boosted scores,Improves relevance ranking,None,,,
Constraint Solver,Tolerance Matcher,Minimal,N/A,Minimal,Built-in,N/A,Numeric constraint satisfaction,core_system.py,Tolerance-based matching,Numeric constraints,Boolean satisfaction,Flexible constraint handling,None,,,
Session Manager,Context Storage,Minimal,N/A,Variable,Built-in,N/A,Conversation context tracking,core_system.py,Dictionary storage,Conversation history,Context information,Maintains conversation state,None,,,
Response Generator,Template Engine,Minimal,N/A,Minimal,Built-in,N/A,Response formatting,core_system.py,String templating,Search results,Formatted responses,User-friendly response generation,None,,,
Vocabulary Manager,Term Indexer,Small,N/A,Variable,Built-in,N/A,Term frequency analysis,utils.py,Dictionary indexing,Document corpus,Term statistics,Efficient term lookup,None,,,
Data Optimizer,Category Classifier,Small,N/A,Minimal,Built-in,N/A,Query categorization,core_system.py,Rule-based classification,Query features,Category labels,Optimizes search strategy,None,,,
Hybrid Search,Multi-modal Retriever,Medium,N/A,Variable,Combined,N/A,Multi-strategy search combination,core_system.py,Score fusion,Multiple search results,Unified ranked results,Best of multiple search methods,All above,,,
AI Summarizer,MiniLM Summarizer,Small,22.7M,90 MB,sentence-transformers,Latest,Property summarization,core_system.py,Transformer-based summarization,Property documents,Intelligent summaries,AI-powered content summarization,transformers torch,,,
